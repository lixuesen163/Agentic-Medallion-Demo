{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "302fba4b",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# 基于 LangChain 智能体的 Medallion 架构数据管道\n",
        "使用 GenAI 驱动的 ETL 编排构建现代数据湖仓\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e106c5d9",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 1：安装依赖并重启 Python\n",
        "本单元格用于安装数据管道所需的全部 Python 库。构建和运行智能体管道需要 langchain、langchain-community、langchain-openai、langgraph、playwright 和 Pillow。在 Databricks 笔记本中，dbutils.library.restartPython() 至关重要，可确保新安装的库立即可用。\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b43fb25",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%pip install -U \\\n",
        "  langchain==0.3.27 \\\n",
        "  langchain-community==0.3.27 \\\n",
        "  langchain-openai>=0.2.0 \\\n",
        "  langgraph==0.6.6 \\\n",
        "  Pillow==10.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1deca6f5",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 2：配置 LangChain 与 LangSmith\n",
        "本单元格设置 LangChain 和 LangSmith 的环境变量。LangSmith 用于对 LangChain 应用进行追踪、监控与调试。需配置端点、API 密钥（请替换为您的实际密钥）以及用于组织运行记录的项目名称。\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d0b3ca",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Azure OpenAI\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://lixuesenuseno2.openai.azure.com/\"\n",
        "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"LXS-gpt-5\"\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"XX\")  # 建议通过环境变量注入真实密钥\n",
        "\n",
        "print(f\"Azure OpenAI endpoint: {os.environ['AZURE_OPENAI_ENDPOINT']}, deployment: {os.environ['AZURE_OPENAI_DEPLOYMENT_NAME']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbed674e",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 3：导入依赖\n",
        "通过完整的库导入搭建基础环境，包括：\n",
        "- LangChain 与 Databricks 的集成，用于基于 LLM 的智能体\n",
        "- LangGraph 组件，用于编排智能体工作流\n",
        "- PySpark SQL 能力，用于可扩展的数据转换\n",
        "- 核心数据科学库，用于分析与可视化\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0241ec9",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "from typing import Annotated, TypedDict, List, Optional\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, regexp_replace, when, lit, md5, concat_ws, expr, to_date, upper, sum as _sum, avg as _avg, count as _count, date_format, year, month\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "364e876d",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 4：初始化 Spark 会话、LLM 与数据库 Schema\n",
        "\n",
        "本单元格初始化 SparkSession、用于智能体操作的大语言模型（LLM），并按照 Medallion 架构模式创建所需的数据库 Schema（Bronze、Silver、Gold）。同时在此定义 log_event 工具函数，用于在管道执行过程中输出结构化、带颜色的日志。\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f256bba8",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def log_event(level, source, message):\n",
        "    colors = {\"AGENT\": \"\\033[94m\", \"TOOL\": \"\\033[93m\", \"INFO\": \"\\033[92m\", \"ERROR\": \"\\033[91m\", \"GRAPH\": \"\\033[95m\"}\n",
        "    reset_color = \"\\033[0m\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
        "    print(f\"{timestamp} | {colors.get(level, '')}{level:<7}{reset_color} | {colors.get(level, '')}[{source}]{reset_color} {message}\")\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AgenticDataPipeline\").getOrCreate()\n",
        "\n",
        "# llm = AzureChatOpenAI(\n",
        "#     azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://lixuesenuseno2.openai.azure.com/\"),\n",
        "#     api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"XX\"),\n",
        "#     api_version=\"2025-01-01-preview\",\n",
        "#     azure_deployment=\"gpt-4.1\",\n",
        "#     temperature=0.0,\n",
        "#     max_tokens=8000,\n",
        "# )\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://lixuesenuseno2.openai.azure.com/\"),\n",
        "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"XX\"),\n",
        "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2025-03-01-preview\"),\n",
        "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"LXS-gpt-5\"),\n",
        "    max_completion_tokens=8000,\n",
        ")\n",
        "\n",
        "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ops_bronze\")\n",
        "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ops_silver\")\n",
        "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ops_gold\")\n",
        "\n",
        "log_event(\"INFO\", \"Setup\", \"Environment initialized with Azure OpenAI (LXS-gpt-5)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5238b1a3",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 5：生成 Bronze 层原始数据\n",
        "\n",
        "本单元格为 Bronze 层创建合成原始数据，在 ops_bronze schema 下生成 customers_raw、transactions_raw、accounts_raw 和 opportunities_raw 表的数据。该步骤模拟将原始、可能杂乱的源数据摄入数据湖的过程。\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a43f8194",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def create_bronze_data():\n",
        "    log_event(\"INFO\", \"DataGen\", \"正在生成 Bronze 层数据...\")\n",
        "    \n",
        "    customer_data = []\n",
        "    for i in range(1, 101):\n",
        "        name = f\"FName{i} LName{i}\" if random.random() > 0.1 else None\n",
        "        email = f\"fname{i}.lname{i}@email.com\" if random.random() > 0.1 else \"invalid-email\"\n",
        "        address = f\"{i*10} Main St\" if random.random() > 0.05 else None\n",
        "        customer_data.append((i, name, email, address, f\"2023-01-{random.randint(10, 28)}\"))\n",
        "    \n",
        "    customer_data.extend([\n",
        "        (1, 'John Doe', 'john.doe@email.com', '123 Elm St', '2023-01-15'), \n",
        "        (3, 'Peter Jones', 'peter.jones.dup@email.com', '789 Pine Ln', '2023-01-17')\n",
        "    ])\n",
        "    spark.createDataFrame(customer_data, [\"customer_id\", \"name\", \"email\", \"address\", \"join_date\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.customers_raw\")\n",
        "\n",
        "    transactions_data = []\n",
        "    for i in range(1, 201):\n",
        "        cust_id = random.randint(1, 110)\n",
        "        qty = random.randint(-5, 50)\n",
        "        amount = f\"${random.uniform(5, 1000):.2f}\" if random.random() > 0.05 else None\n",
        "        transactions_data.append((100+i, cust_id, qty, amount, f\"2023-02-{random.randint(1, 28)}\"))\n",
        "    spark.createDataFrame(transactions_data, [\"transaction_id\", \"customer_id\", \"quantity\", \"amount\", \"transaction_date\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.transactions_raw\")\n",
        "\n",
        "    accounts_data = [('ACC{:03d}'.format(i), f'GlobalCorp-{i}', random.choice(['Technology', 'Healthcare', 'Finance', 'TECH', None]), random.choice(['USA', 'UK', 'Germany'])) for i in range(1, 51)]\n",
        "    accounts_data.append(('ACC001', 'Global Corporation', 'Tech', 'USA'))\n",
        "    spark.createDataFrame(accounts_data, [\"account_id\", \"account_name\", \"industry\", \"region\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.accounts_raw\")\n",
        "    \n",
        "    opportunities_data = [(f'OPP{i:03d}', f\"ACC{random.randint(1, 55):03d}\", random.randint(-1000, 200000), '2024-07-15', random.choice(['Closed Won', 'Negotiation', 'Proposal', 'Qualification', 'Closed Lost'])) for i in range(1, 151)]\n",
        "    spark.createDataFrame(opportunities_data, [\"opportunity_id\", \"account_id\", \"value\", \"close_date\", \"stage\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.opportunities_raw\")\n",
        "    \n",
        "    log_event(\"INFO\", \"DataGen\", \"Bronze data creation complete\")\n",
        "    display(spark.sql(\"SHOW TABLES IN ops_bronze\"))\n",
        "\n",
        "create_bronze_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1925d440",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 6：定义智能体工具\n",
        "\n",
        "本单元格定义 LangGraph 工作流中智能体可调用的工具函数：\n",
        "\n",
        "- **get_table_info**：对指定表进行画像，提供 Schema、行数与数据预览。\n",
        "- **execute_pyspark_code**：在 Spark 环境中安全执行生成的 PySpark 代码，并处理错误。\n",
        "- **create_notebook_visualization**：在 Databricks 笔记本内直接生成并展示基础可视化。\n",
        "\n",
        "这些工具使 LLM 智能体能够与数据及环境进行交互。\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba52b67d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def get_table_info(table_name: str) -> str:\n",
        "    \"\"\"对表进行画像：包含 Schema、统计信息与数据预览，用于转换规划。\"\"\"\n",
        "    try:\n",
        "        log_event(\"TOOL\", \"get_table_info\", f\"正在画像: {table_name}\")\n",
        "        df = spark.table(table_name)\n",
        "        schema_str = \"\\n\".join([f\"- {field.name}: {str(field.dataType)}\" for field in df.schema.fields])\n",
        "        count = df.count()\n",
        "        preview = df.limit(3).toPandas().to_string()\n",
        "        return f\"TABLE: {table_name}\\nROW_COUNT: {count}\\n\\nSCHEMA:\\n{schema_str}\\n\\nSAMPLE_DATA:\\n{preview}\"\n",
        "    except Exception as e:\n",
        "        return f\"错误: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def execute_pyspark_code(code: str) -> str:\n",
        "    \"\"\"在具备错误处理与校验的前提下执行 PySpark 转换代码。\"\"\"\n",
        "    try:\n",
        "        log_event(\"TOOL\", \"execute_pyspark_code\", \"正在执行转换...\")\n",
        "        exec_globals = {\n",
        "            'spark': spark, 'col': col, 'regexp_replace': regexp_replace, 'when': when, 'lit': lit,\n",
        "            'md5': md5, 'concat_ws': concat_ws, 'expr': expr, 'to_date': to_date, 'upper': upper,\n",
        "            '_sum': _sum, '_avg': _avg, '_count': _count, 'date_format': date_format, 'year': year, 'month': month, 'Window': Window\n",
        "        }\n",
        "        exec(code, exec_globals)\n",
        "        log_event(\"TOOL\", \"execute_pyspark_code\", \"执行成功\")\n",
        "        return json.dumps({\"status\": \"success\", \"error\": None})\n",
        "    except Exception as e:\n",
        "        error_msg = f\"执行错误: {str(e)}\"\n",
        "        log_event(\"ERROR\", \"execute_pyspark_code\", error_msg)\n",
        "        return json.dumps({\"status\": \"error\", \"error\": error_msg})\n",
        "        \n",
        "@tool\n",
        "def create_notebook_visualization(table_name: str, plot_type: str, x_col: str, y_col: str, title: str) -> str:\n",
        "    \"\"\"创建优化的仪表盘可视化（表格预览 + matplotlib 图），自动排序与限制数据以提升性能。\"\"\"\n",
        "    try:\n",
        "        log_event(\"TOOL\", \"create_visualization\", f\"正在创建: {title}\")\n",
        "\n",
        "        df = spark.table(table_name)\n",
        "\n",
        "        # 保留你原来的逻辑：Top bar -> y desc limit 5，否则按 x 排序\n",
        "        if plot_type == 'bar' and 'Top' in title:\n",
        "            df_display = df.orderBy(col(y_col).desc()).limit(5)\n",
        "        else:\n",
        "            df_display = df.orderBy(col(x_col))\n",
        "\n",
        "        print(f\"\\n=== DASHBOARD COMPONENT: {title} ===\")\n",
        "\n",
        "        # 1) 表格预览（原逻辑不变）\n",
        "        display(df_display)\n",
        "\n",
        "        # 2) 真正画图（新增，但不改变结果表，只是多一个可视化输出）\n",
        "        pdf = df_display.select(col(x_col).alias(\"x\"), col(y_col).alias(\"y\")).toPandas()\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # x 轴转成字符串，避免 category/日期等类型导致画图异常\n",
        "        pdf[\"x\"] = pdf[\"x\"].astype(str)\n",
        "\n",
        "        plt.figure()\n",
        "        if plot_type == \"bar\":\n",
        "            plt.bar(pdf[\"x\"], pdf[\"y\"])\n",
        "            plt.xticks(rotation=30, ha=\"right\")\n",
        "        elif plot_type == \"line\":\n",
        "            plt.plot(pdf[\"x\"], pdf[\"y\"], marker=\"o\")\n",
        "            plt.xticks(rotation=30, ha=\"right\")\n",
        "        else:\n",
        "            return f\"VISUALIZATION_ERROR: Unsupported plot_type='{plot_type}'. Use 'bar' or 'line'.\"\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.xlabel(x_col)\n",
        "        plt.ylabel(y_col)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return f\"SUCCESS: '{title}' visualization created and displayed\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"VISUALIZATION_ERROR: {str(e)}\"\n",
        "\n",
        "all_tools = [get_table_info, execute_pyspark_code, create_notebook_visualization]\n",
        "tool_node = ToolNode(all_tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a44a1d",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 7：定义智能体状态、节点与图工作流\n",
        "\n",
        "这是智能体管道的核心部分。\n",
        "\n",
        "### AgentState（智能体状态）\n",
        "定义在 LangGraph 工作流中在智能体之间传递的共享状态对象，包括消息、当前任务、PySpark 代码、审查反馈、执行错误与重试次数。\n",
        "\n",
        "### 智能体函数\n",
        "\n",
        "- **planner_agent**：生成详细的数据转换方案\n",
        "- **code_generator_agent**：根据方案与反馈编写 PySpark 代码\n",
        "- **code_reviewer_agent**：对生成的代码进行质量与正确性审查\n",
        "- **prepare_for_execution**：将 PySpark 代码格式化为可供工具执行的形式\n",
        "\n",
        "### 条件边\n",
        "- **after_review_decider**：根据代码审查反馈进行路由\n",
        "- **after_execution_decider**：根据执行结果进行路由\n",
        "二者共同实现自主自纠循环。\n",
        "\n",
        "### 工作流定义\n",
        "StateGraph 定义 Medallion 管道工作流的节点（智能体与工具）与边（状态转移）。\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc59496",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, lambda x, y: x + y]\n",
        "    current_task: str\n",
        "    pyspark_code: Optional[str]\n",
        "    review_feedback: Optional[str]\n",
        "    execution_error: Optional[str]\n",
        "    retry_count: int\n",
        "\n",
        "def planner_agent(state: AgentState):\n",
        "    log_event(\"AGENT\", \"Planner\", \"正在制定转换策略\")\n",
        "    \n",
        "    system_prompt = \"\"\"You are an expert data architect specializing in Databricks Medallion architecture. \n",
        "    \n",
        "    OBJECTIVES:\n",
        "    - Create precise, executable PySpark transformation plans\n",
        "    - Ensure data quality, deduplication, and proper type casting\n",
        "    - Follow medallion best practices (Bronze=raw, Silver=cleaned, Gold=aggregated)\n",
        "    - Design for scalability and performance optimization\n",
        "    \n",
        "    OUTPUT FORMAT:\n",
        "    Provide a detailed, step-by-step technical plan with:\n",
        "    1. Data profiling requirements\n",
        "    2. Specific transformation logic\n",
        "    3. Quality checks and validation rules\n",
        "    4. Performance optimization strategies\"\"\"\n",
        "    \n",
        "    user_prompt = f\"\"\"转换任务: {state['current_task']}\n",
        "    \n",
        "    请制定完整的技术方案，涵盖：\n",
        "    - 数据质量问题与修复\n",
        "    - Schema 标准化与类型转换\n",
        "    - 去重策略\n",
        "    - 业务规则实现\n",
        "    - 性能优化手段\"\"\"\n",
        "    \n",
        "    messages = [HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")]\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"messages\": [AIMessage(content=response.content, name=\"PlannerAgent\")]}\n",
        "\n",
        "def code_generator_agent(state: AgentState):\n",
        "    log_event(\"AGENT\", \"CodeGenerator\", \"正在生成 PySpark 实现\")\n",
        "\n",
        "    # 取 Planner 的方案（如果没有也要容错）\n",
        "    plan = next(\n",
        "        (msg.content for msg in state.get(\"messages\", []) \n",
        "         if isinstance(msg, AIMessage) and getattr(msg, \"name\", \"\") == \"PlannerAgent\"),\n",
        "        \"\"\n",
        "    )\n",
        "\n",
        "    # 拼接上下文：优先采纳 review feedback，其次修复 execution error\n",
        "    context_prompt = \"\"\n",
        "    if state.get(\"review_feedback\") and \"APPROVED\" not in (state[\"review_feedback\"] or \"\").upper():\n",
        "        context_prompt = f\"\\n\\nREVIEW_FEEDBACK_TO_APPLY:\\n{state['review_feedback']}\"\n",
        "    elif state.get(\"execution_error\"):\n",
        "        context_prompt = f\"\\n\\nEXECUTION_ERROR_TO_FIX:\\n{state['execution_error']}\"\n",
        "\n",
        "    system_prompt = \"\"\"You are a senior PySpark developer building Databricks Medallion transformations.\n",
        "\n",
        "HARD REQUIREMENTS (must follow exactly):\n",
        "1) OUTPUT ONLY executable Python code. No markdown fences, no comments, no explanations.\n",
        "2) The SparkSession variable `spark` is already available.\n",
        "3) Read from Bronze tables and write to Silver tables using:\n",
        "   df.write.mode(\"overwrite\").saveAsTable(\"<target_table>\")\n",
        "4) Include explicit imports for every PySpark function you use (from pyspark.sql.functions / types / window).\n",
        "5) Code must be deterministic and runnable on Databricks Serverless compute.\n",
        "\n",
        "SERVERLESS CONSTRAINTS (critical):\n",
        "- DO NOT use Spark SQL table caching or persistence commands:\n",
        "  CACHE TABLE, UNCACHE TABLE, PERSIST TABLE, CLEAR CACHE\n",
        "- DO NOT use spark.catalog.cacheTable / uncacheTable\n",
        "- Avoid any SQL statements that may translate to table-level persistence.\n",
        "\n",
        "PERFORMANCE (allowed optimizations that DO NOT change results):\n",
        "- Use column pruning, filter pushdown, select only needed columns\n",
        "- Avoid wide shuffles when not necessary\n",
        "- Use DataFrame operations; if caching is considered, skip it (do not cache tables)\n",
        "\n",
        "ROBUSTNESS:\n",
        "- Handle nulls and malformed values safely\n",
        "- Ensure type casting is safe (use when/otherwise, regexp, to_date, etc.)\n",
        "- Deduplicate using Window + row_number or dropDuplicates with deterministic ordering\n",
        "- If a column required by logic is missing, fail with a clear exception message.\n",
        "- Before writing code, infer column names from the task text. NEVER invent required columns.\n",
        "- For opportunities_raw, the numeric field is `value`. Create `opportunity_value` via withColumn, do not require it to exist.\n",
        "\n",
        "OUTPUT CONTRACT:\n",
        "- Must create exactly one target Silver table for the given task.\n",
        "- Must not create temp views that replace the target contract.\n",
        "\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"TASK:\n",
        "{state.get('current_task', '')}\n",
        "\n",
        "IMPLEMENTATION PLAN:\n",
        "{plan}{context_prompt}\n",
        "\n",
        "Generate the complete PySpark code that implements the task exactly.\n",
        "Remember: output ONLY Python code, no markdown, no commentary.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")]\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    # 清理模型可能输出的代码块围栏（兜底）\n",
        "    clean_code = (\n",
        "        response.content.strip()\n",
        "        .replace(\"```python\", \"\")\n",
        "        .replace(\"```\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"pyspark_code\": clean_code,\n",
        "        \"execution_error\": None,\n",
        "        \"review_feedback\": None\n",
        "    }\n",
        "\n",
        "def code_reviewer_agent(state: AgentState):\n",
        "    log_event(\"AGENT\", \"CodeReviewer\", \"正在进行质量审查\")\n",
        "\n",
        "    plan = next(\n",
        "        (msg.content for msg in state.get(\"messages\", [])\n",
        "         if isinstance(msg, AIMessage) and getattr(msg, \"name\", \"\") == \"PlannerAgent\"),\n",
        "        \"\"\n",
        "    )\n",
        "\n",
        "    system_prompt = \"\"\"You are a senior Data Engineering QA reviewer for PySpark on Databricks.\n",
        "\n",
        "You must strictly enforce correctness and Serverless compatibility.\n",
        "\n",
        "REVIEW CHECKLIST (fail if any item is violated):\n",
        "A) Output is pure Python code (no markdown fences, no explanations).\n",
        "B) Uses SparkSession `spark` (assume pre-initialized) and reads from the specified source table(s).\n",
        "C) Writes EXACTLY one target Silver table using:\n",
        "   .write.mode(\"overwrite\").saveAsTable(\"<target_table>\")\n",
        "D) Includes explicit imports for every used PySpark function/type/window.\n",
        "E) Implements all data-quality rules from the plan/task (dedup, type casting, filtering, validations, flags).\n",
        "F) Serverless constraints:\n",
        "   - Must NOT contain: CACHE TABLE, UNCACHE TABLE, PERSIST TABLE, CLEAR CACHE\n",
        "   - Must NOT call spark.catalog.cacheTable / uncacheTable\n",
        "G) No dangerous dynamic execution or notebook magic (no %sql, no dbutils.fs, no system calls).\n",
        "\n",
        "RESPONSE FORMAT (must follow exactly):\n",
        "- If ALL checks pass: respond with exactly: APPROVED\n",
        "- Otherwise: respond with:\n",
        "  REJECTION_REASON: <bullet list of specific actionable fixes>\n",
        "\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"TASK:\n",
        "{state.get('current_task', '')}\n",
        "\n",
        "PLAN:\n",
        "{plan}\n",
        "\n",
        "CODE TO REVIEW:\n",
        "{state.get('pyspark_code', '')}\n",
        "\"\"\"\n",
        "\n",
        "    messages = [HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")]\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    return {\"review_feedback\": response.content}\n",
        "\n",
        "\n",
        "def prepare_for_execution(state: AgentState):\n",
        "    log_event(\"AGENT\", \"ExecutorPrep\", \"准备执行代码\")\n",
        "    tool_call_message = AIMessage(\n",
        "        content=\"\", \n",
        "        tool_calls=[{\n",
        "            'name': 'execute_pyspark_code', \n",
        "            'args': {'code': state.get('pyspark_code')}, \n",
        "            'id': f'exec_{datetime.now().isoformat()}'\n",
        "        }]\n",
        "    )\n",
        "    return {\"messages\": [tool_call_message]}\n",
        "\n",
        "def after_review_decider(state: AgentState):\n",
        "    feedback = state.get('review_feedback', '')\n",
        "    if \"APPROVED\" in feedback.upper():\n",
        "        log_event(\"GRAPH\", \"Router\", \"代码已通过 → 执行\")\n",
        "        return \"prepare_for_execution\"\n",
        "    else:\n",
        "        log_event(\"GRAPH\", \"Router\", f\"代码未通过 → 修订（第 {state.get('retry_count', 0) + 1} 次尝试）\")\n",
        "        return \"revise_code\"\n",
        "\n",
        "def after_execution_decider(state: AgentState):\n",
        "    tool_result = _parse_last_tool_result(state) or {\"status\": \"error\", \"error\": \"No tool result found\"}\n",
        "\n",
        "    if tool_result.get(\"status\") == \"success\":\n",
        "        log_event(\"GRAPH\", \"Router\", \"执行成功 → 完成\")\n",
        "        return \"__end__\"\n",
        "\n",
        "    # 失败：最多重试 3 次（与原逻辑含义一致：retry_count < 3 继续）\n",
        "    next_retry = state.get(\"retry_count\", 0) + 1\n",
        "    if next_retry <= 3:\n",
        "        log_event(\"GRAPH\", \"Router\", f\"执行失败 → 重试（第 {next_retry} 次）\")\n",
        "        return \"revise_code\"\n",
        "\n",
        "    log_event(\"ERROR\", \"Router\", \"已达最大重试次数 → 中止\")\n",
        "    return \"__end__\"\n",
        "\n",
        "def _parse_last_tool_result(state: AgentState):\n",
        "    \"\"\"只解析 messages[-1]，避免把历史 tool error 误当成当前错误\"\"\"\n",
        "    if not state.get(\"messages\"):\n",
        "        return None\n",
        "    last = state[\"messages\"][-1]\n",
        "    content = getattr(last, \"content\", None)\n",
        "    if not content:\n",
        "        return None\n",
        "    try:\n",
        "        obj = json.loads(content)\n",
        "        if isinstance(obj, dict) and \"status\" in obj:\n",
        "            return obj\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "def revise_code_node_fn(state: AgentState):\n",
        "    update = {\"retry_count\": state.get(\"retry_count\", 0) + 1}\n",
        "\n",
        "    # 如果刚刚执行过工具（executor 的输出通常是 JSON），把 error 写入 state 让下一轮生成代码能用\n",
        "    tool_result = _parse_last_tool_result(state)\n",
        "    if tool_result and tool_result.get(\"status\") == \"error\":\n",
        "        update[\"execution_error\"] = tool_result.get(\"error\")\n",
        "\n",
        "    return update\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"planner\", planner_agent)\n",
        "workflow.add_node(\"code_generator\", code_generator_agent)\n",
        "workflow.add_node(\"code_reviewer\", code_reviewer_agent)\n",
        "workflow.add_node(\"revise_code_node\", revise_code_node_fn)\n",
        "workflow.add_node(\"prepare_for_execution\", prepare_for_execution)\n",
        "workflow.add_node(\"executor\", tool_node)\n",
        "\n",
        "workflow.set_entry_point(\"planner\")\n",
        "workflow.add_edge(\"planner\", \"code_generator\")\n",
        "workflow.add_edge(\"code_generator\", \"code_reviewer\")\n",
        "workflow.add_conditional_edges(\"code_reviewer\", after_review_decider, {\n",
        "    \"prepare_for_execution\": \"prepare_for_execution\", \n",
        "    \"revise_code\": \"revise_code_node\"\n",
        "})\n",
        "workflow.add_edge(\"revise_code_node\", \"code_generator\")\n",
        "workflow.add_edge(\"prepare_for_execution\", \"executor\")\n",
        "workflow.add_conditional_edges(\"executor\", after_execution_decider, {\n",
        "    \"revise_code\": \"revise_code_node\",\n",
        "    \"__end__\": END\n",
        "})\n",
        "\n",
        "app = workflow.compile()\n",
        "log_event(\"INFO\", \"Setup\", \"Enhanced LangGraph pipeline compiled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac837680",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def create_workflow_diagram():\n",
        "    \"\"\"当可视化不可用时，生成基于文本的工作流图作为备选\"\"\"\n",
        "    diagram = \"\"\"\n",
        "    ┌─────────────┐\n",
        "    │   PLANNER   │\n",
        "    │   AGENT     │\n",
        "    └─────┬───────┘\n",
        "          │\n",
        "          ▼\n",
        "    ┌─────────────┐\n",
        "    │    CODE     │\n",
        "    │ GENERATOR   │\n",
        "    └─────┬───────┘\n",
        "          │\n",
        "          ▼\n",
        "    ┌─────────────┐\n",
        "    │    CODE     │\n",
        "    │  REVIEWER   │\n",
        "    └─────┬───────┘\n",
        "          │\n",
        "          ▼\n",
        "    ┌─────────────┐    ┌─────────────┐\n",
        "    │  APPROVED?  │───►│   REVISE    │\n",
        "    │             │    │    CODE     │\n",
        "    └─────┬───────┘    └─────┬───────┘\n",
        "          │                  │\n",
        "          ▼                  │\n",
        "    ┌─────────────┐          │\n",
        "    │  EXECUTE    │◄─────────┘\n",
        "    │    CODE     │\n",
        "    └─────┬───────┘\n",
        "          │\n",
        "          ▼\n",
        "    ┌─────────────┐\n",
        "    │  SUCCESS?   │\n",
        "    │             │\n",
        "    └─────────────┘\n",
        "    \"\"\"\n",
        "    print(\"=== LangGraph 工作流图 ===\")\n",
        "    print(diagram)\n",
        "\n",
        "try:\n",
        "    # Try to create visual graph with improved error handling\n",
        "    try:\n",
        "        # First try with mermaid text output (safer fallback)\n",
        "        mermaid_code = app.get_graph().draw_mermaid()\n",
        "        print(\"=== WORKFLOW MERMAID DIAGRAM ===\")\n",
        "        print(mermaid_code)\n",
        "    except Exception as mermaid_error:\n",
        "        log_event(\"INFO\", \"Visualization\", f\"Mermaid generation failed: {mermaid_error}\")\n",
        "        \n",
        "    # Try PNG generation with better error handling\n",
        "    try:\n",
        "        from langgraph.graph.graph import CompiledGraph\n",
        "        if hasattr(app.get_graph(), 'draw_ascii'):\n",
        "            ascii_graph = app.get_graph().draw_ascii()\n",
        "            print(\"=== WORKFLOW ASCII DIAGRAM ===\")\n",
        "            print(ascii_graph)\n",
        "        else:\n",
        "            create_workflow_diagram()\n",
        "    except Exception as png_error:\n",
        "        log_event(\"INFO\", \"Visualization\", f\"PNG visualization unavailable: {png_error}\")\n",
        "        create_workflow_diagram()\n",
        "        \n",
        "except Exception as e:\n",
        "    log_event(\"INFO\", \"Visualization\", \"Using text-based workflow diagram\")\n",
        "    create_workflow_diagram()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72ec4c76",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 9：执行 Bronze 到 Silver 的转换\n",
        "\n",
        "本单元格启动 Medallion 管道的第一个主要阶段：将 Bronze 层的原始数据转换为清洗、标准化后的 Silver 层。它会遍历预定义的转换任务列表，每个任务描述对特定表（客户、交易、账户、商机）进行数据清洗与校验的要求。execute_pipeline_task 函数为每个任务编排智能体工作流。\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf5c124",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def execute_pipeline_task(task_description):\n",
        "    task_name = task_description.strip().splitlines()[0][:50]\n",
        "    log_event(\"INFO\", \"Pipeline\", f\"正在执行: {task_name}\")\n",
        "    \n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=\"初始化转换管道\")], \n",
        "        \"current_task\": task_description, \n",
        "        \"retry_count\": 0\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        for state_update in app.stream(initial_state, {\"recursion_limit\": 80}):\n",
        "            node_name = list(state_update.keys())[0]\n",
        "            log_event(\"GRAPH\", \"Flow\", f\"已完成: {node_name}\")\n",
        "        \n",
        "        log_event(\"INFO\", \"Pipeline\", f\"已完成: {task_name}\")\n",
        "    except Exception as e:\n",
        "        log_event(\"ERROR\", \"Pipeline\", f\"任务失败: {task_name} - {str(e)}\")\n",
        "    \n",
        "    time.sleep(1)\n",
        "\n",
        "log_event(\"INFO\", \"PIPELINE\", \"===== BRONZE → SILVER TRANSFORMATIONS =====\")\n",
        "\n",
        "bronze_to_silver_transformations = [\n",
        "    \"\"\"CUSTOMER DATA CLEANSING: ops_bronze.customers_raw → ops_silver.customers_cleaned\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Remove duplicate customer_id records (keep first occurrence)  \n",
        "    - Fill null 'name' values with 'Unknown Customer'\n",
        "    - Validate email format using regex pattern '.+@.+\\\\..+' \n",
        "    - Set invalid emails to null\n",
        "    - Convert join_date string to proper DateType\n",
        "    - Filter out records with null email OR null address\n",
        "    - Add data quality flags for tracking\"\"\",\n",
        "    \n",
        "    \"\"\"TRANSACTION DATA STANDARDIZATION: ops_bronze.transactions_raw → ops_silver.transactions_cleaned\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Deduplicate on transaction_id (keep first record)\n",
        "    - Clean amount field: remove '$' prefix and convert to Decimal(10,2)\n",
        "    - Filter out negative quantity transactions\n",
        "    - Filter out null or zero amount transactions  \n",
        "    - Restrict to customer_id <= 100 (valid customers only)\n",
        "    - Convert transaction_date to DateType\n",
        "    - Add calculated fields for analysis\"\"\",\n",
        "    \n",
        "    \"\"\"ACCOUNT DATA NORMALIZATION: ops_bronze.accounts_raw → ops_silver.accounts_cleaned\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Deduplicate by account_id, preserving first record\n",
        "    - Standardize industry values to uppercase\n",
        "    - Map 'TECH' industry to 'TECHNOLOGY' \n",
        "    - Replace null industry with 'NOT_SPECIFIED'\n",
        "    - Validate account_id format consistency\n",
        "    - Add data lineage tracking columns\"\"\",\n",
        "    \n",
        "    \"\"\"OPPORTUNITY DATA VALIDATION: ops_bronze.opportunities_raw → ops_silver.opportunities_cleaned\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Source schema columns are exactly: opportunity_id, account_id, value, close_date, stage\n",
        "    - Use source column name `value` (DO NOT assume `opportunity_value` exists). If you create a new column, name it `opportunity_value`.\n",
        "    - Cast opportunity value to Decimal(14,2) with proper handling\n",
        "    - Filter out opportunities with value <= 0\n",
        "    - Validate account_id matches pattern 'ACC###'\n",
        "    - Convert close_date string to DateType\n",
        "    - Standardize stage values with proper casing\n",
        "    - Add business rule validation flags\"\"\"\n",
        "]\n",
        "\n",
        "for task in bronze_to_silver_transformations:\n",
        "    execute_pipeline_task(task)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76a96a95",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 10：执行 Silver 到 Gold 的聚合\n",
        "\n",
        "本单元格执行 Medallion 管道的第二个主要阶段：将 Silver 层的数据聚合并丰富到 Gold 层。它定义并执行创建分析表（如 customer_spending、account_performance、monthly_sales_summary）的任务。这些聚合为商业智能与报表提供数据准备。\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42cd24b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "log_event(\"INFO\", \"PIPELINE\", \"===== SILVER → GOLD AGGREGATIONS =====\")\n",
        "\n",
        "silver_to_gold_aggregations = [ \n",
        "    \"\"\"CUSTOMER SPENDING ANALYTICS: Create ops_gold.customer_spending\n",
        "    SOURCE SCHEMA NOTE:\n",
        "     - ops_silver.customers_cleaned columns include: customer_id, name, email, address, join_date (and possibly others)\n",
        "     - DO NOT require or reference audit fields (effective_ts, updated_at) unless explicitly present.\n",
        "    REQUIREMENTS:\n",
        "     - Join ops_silver.customers_cleaned with ops_silver.transactions_cleaned\n",
        "     - Group by the `customer_id` and `name` columns of the `customers_cleaned` table.\n",
        "     - Calculate total_spent (sum of amounts) with null handling\n",
        "     - Calculate total_transactions (count of transactions)\n",
        "     - Calculate average_transaction_value \n",
        "     - Add customer spending tier classification\n",
        "     - Order by total_spent descending for performance\n",
        "     \"\"\",      \n",
        "    \"\"\"SOURCE SCHEMA NOTE:\n",
        "- ops_silver.accounts_cleaned columns (types):\n",
        "  - account_id (string)\n",
        "  - account_name (string)\n",
        "  - industry (string)\n",
        "  - lineage_source (string)\n",
        "  - lineage_processed_at (timestamp)\n",
        "\n",
        "- ops_silver.opportunities_cleaned columns (types):\n",
        "  - opportunity_id (string)\n",
        "  - account_id (string)\n",
        "  - opportunity_value (decimal(14,2))\n",
        "  - close_date (date)\n",
        "  - stage (string)\n",
        "  - is_valid_value (boolean)\n",
        "  - is_valid_account_id (boolean)\n",
        "  - is_valid_close_date (boolean)\n",
        "  - is_valid_stage (boolean)\n",
        "\n",
        "IMPORTANT CONSTRAINTS:\n",
        "- Use opportunity_value as the numeric field for all sums (do NOT use `value`).\n",
        "- Join key is account_id (string) between the two tables.\n",
        "- stage may have inconsistent casing; normalize before comparisons for 'Closed Won' and closed-stage logic.\n",
        "- Validation flags exist but are optional for metrics; do not require any additional audit columns not listed above.\n",
        "\n",
        "ACCOUNT PERFORMANCE METRICS: Create ops_gold.account_performance \n",
        "REQUIREMENTS: \n",
        "- Join ops_silver.accounts_cleaned with ops_silver.opportunities_cleaned \n",
        "- Group by account_id, account_name, and industry \n",
        "- Calculate total_pipeline_value (sum of all opportunity values) \n",
        "- Calculate won_value (sum where stage = 'Closed Won') \n",
        "- Calculate win_rate as percentage of closed won vs total closed \n",
        "- Count open_opportunities (non-closed stages) \n",
        "- Add performance ranking within industry\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "MONTHLY SALES TRENDS: Create ops_gold.monthly_sales_summary\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Source from ops_silver.transactions_cleaned\n",
        "    - Extract year-month from transaction_date as 'YYYY-MM' format\n",
        "    - Group by month period  \n",
        "    - Calculate monthly_revenue (sum of amounts)\n",
        "    - Calculate monthly_transaction_count\n",
        "    - Calculate average_transaction_size per month\n",
        "    - Add month-over-month growth calculations\n",
        "    - Order chronologically for time series analysis\n",
        "\"\"\"\n",
        "]\n",
        "\n",
        "for task in silver_to_gold_aggregations:\n",
        "    execute_pipeline_task(task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e368e267",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "log_event(\"INFO\", \"PIPELINE\", \"===== BUSINESS INTELLIGENCE DASHBOARD =====\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a91e03",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## 单元格 12：生成并校验仪表盘可视化\n",
        "\n",
        "本单元格利用 bi_agent（使用 Notebook_visualization 工具）生成关键商业智能仪表盘组件，随后对 Gold 层表进行最终数据校验，通过展示记录数与 Top N 行，确保管道输出符合预期。\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659bcaf0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# dashboard_prompt = \"\"\"你是商业智能（BI）专家。你必须使用 create_notebook_visualization 工具在 Databricks Notebook 中创建 3 个组件。\n",
        "# 严格要求：\n",
        "# - 你必须调用 create_notebook_visualization 恰好 3 次。\n",
        "# - 不允许调用 get_table_info、execute_pyspark_code 或任何其他工具。\n",
        "# - 每次调用必须提供完整参数：table_name, plot_type, x_col, y_col, title。\n",
        "# - 只输出工具调用，不要输出解释性文字。\n",
        "\n",
        "# 要创建的 3 个可视化组件如下（按顺序）：\n",
        "\n",
        "# 1) 客户分析（柱状图，Top 5）：\n",
        "# - table_name = ops_gold.customer_spending\n",
        "# - plot_type = bar\n",
        "# - x_col = name\n",
        "# - y_col = total_spent\n",
        "# - title = Top 5 Customers by Total Spent\n",
        "\n",
        "# 2) 行业绩效（柱状图，按行业分布）：\n",
        "# - table_name = ops_gold.account_performance\n",
        "# - plot_type = bar\n",
        "# - x_col = industry\n",
        "# - y_col = total_pipeline_value\n",
        "# - title = Pipeline Value by Industry\n",
        "\n",
        "# 3) 收入趋势（折线图）：\n",
        "# - table_name = ops_gold.monthly_sales_summary\n",
        "# - plot_type = line\n",
        "# - x_col = month\n",
        "# - y_col = monthly_revenue\n",
        "# - title = Monthly Revenue Trend\n",
        "# \"\"\"\n",
        "# print([t.name for t in all_tools])\n",
        "# bi_agent = llm.bind_tools(all_tools)\n",
        "# dashboard_response = bi_agent.invoke(dashboard_prompt)\n",
        "\n",
        "# if dashboard_response.tool_calls:\n",
        "#     for tool_call in dashboard_response.tool_calls:\n",
        "#         tool_function = {t.name: t for t in all_tools}[tool_call['name']]\n",
        "#         result = tool_function.invoke(tool_call['args'])\n",
        "#         log_event(\"INFO\", \"Dashboard\", f\"已创建可视化: {tool_call['args'].get('title', 'Unknown')}\")\n",
        "# else:\n",
        "#     log_event(\"ERROR\", \"Dashboard\", \"未能生成仪表盘可视化\")\n",
        "\n",
        "# log_event(\"INFO\", \"VALIDATION\", \"===== 最终数据校验 =====\")\n",
        "\n",
        "# validation_tables = [\n",
        "#     \"ops_gold.customer_spending\",\n",
        "#     \"ops_gold.account_performance\", \n",
        "#     \"ops_gold.monthly_sales_summary\"\n",
        "# ]\n",
        "\n",
        "# for table in validation_tables:\n",
        "#     try:\n",
        "#         print(f\"\\n=== 最终校验: {table} ===\")\n",
        "#         df = spark.table(table)\n",
        "#         print(f\"Record Count: {df.count()}\")\n",
        "#         if \"customer_spending\" in table:\n",
        "#             display(df.orderBy(col(\"total_spent\").desc()).limit(10))\n",
        "#         elif \"account_performance\" in table:\n",
        "#             display(df.orderBy(col(\"total_pipeline_value\").desc()).limit(10))\n",
        "#         else:\n",
        "#             display(df.orderBy(\"month\").limit(12))\n",
        "#     except Exception as e:\n",
        "#         log_event(\"ERROR\", \"Validation\", f\"校验 {table} 失败: {str(e)}\")\n",
        "\n",
        "# log_event(\"INFO\", \"COMPLETION\", \"===== Medallion 管道执行完成 =====\")\n",
        "\n",
        "\n",
        "create_notebook_visualization.invoke({\n",
        "  \"table_name\":\"ops_gold.customer_spending\",\n",
        "  \"plot_type\":\"bar\",\n",
        "  \"x_col\":\"name\",\n",
        "  \"y_col\":\"total_spent\",\n",
        "  \"title\":\"Top 5 Customers by Total Spent\"\n",
        "})\n",
        "\n",
        "create_notebook_visualization.invoke({\n",
        "  \"table_name\":\"ops_gold.account_performance\",\n",
        "  \"plot_type\":\"bar\",\n",
        "  \"x_col\":\"industry\",\n",
        "  \"y_col\":\"total_pipeline_value\",\n",
        "  \"title\":\"Pipeline Value by Industry\"\n",
        "})\n",
        "\n",
        "create_notebook_visualization.invoke({\n",
        "  \"table_name\":\"ops_gold.monthly_sales_summary\",\n",
        "  \"plot_type\":\"line\",\n",
        "  \"x_col\":\"month\",\n",
        "  \"y_col\":\"monthly_revenue\",\n",
        "  \"title\":\"Monthly Revenue Trend\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f548d91",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "print(\"\\n🎉 SUCCESS: Databricks Medallion Architecture pipeline completed successfully!\")\n",
        "print(\"✅ All Bronze → Silver → Gold transformations executed\")\n",
        "print(\"✅ Business Intelligence dashboard components created\")\n",
        "print(\"✅ Data quality validation completed\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
