{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "302fba4b",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# åŸºäº LangChain æ™ºèƒ½ä½“çš„ Medallion æ¶æ„æ•°æ®ç®¡é“\n",
        "ä½¿ç”¨ GenAI é©±åŠ¨çš„ ETL ç¼–æ’æ„å»ºç°ä»£æ•°æ®æ¹–ä»“\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e106c5d9",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 1ï¼šå®‰è£…ä¾èµ–å¹¶é‡å¯ Python\n",
        "æœ¬å•å…ƒæ ¼ç”¨äºå®‰è£…æ•°æ®ç®¡é“æ‰€éœ€çš„å…¨éƒ¨ Python åº“ã€‚æ„å»ºå’Œè¿è¡Œæ™ºèƒ½ä½“ç®¡é“éœ€è¦ langchainã€langchain-communityã€langchain-openaiã€langgraphã€playwright å’Œ Pillowã€‚åœ¨ Databricks ç¬”è®°æœ¬ä¸­ï¼Œdbutils.library.restartPython() è‡³å…³é‡è¦ï¼Œå¯ç¡®ä¿æ–°å®‰è£…çš„åº“ç«‹å³å¯ç”¨ã€‚\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b43fb25",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%pip install langchain==0.3.27 langchain-community==0.3.27 langchain-openai>=0.2.0 langgraph==0.6.6 typing_extensions>=4.8.0 playwright==1.45.0 Pillow==10.4.0\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1deca6f5",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 2ï¼šé…ç½® LangChain ä¸ LangSmith\n",
        "æœ¬å•å…ƒæ ¼è®¾ç½® LangChain å’Œ LangSmith çš„ç¯å¢ƒå˜é‡ã€‚LangSmith ç”¨äºå¯¹ LangChain åº”ç”¨è¿›è¡Œè¿½è¸ªã€ç›‘æ§ä¸è°ƒè¯•ã€‚éœ€é…ç½®ç«¯ç‚¹ã€API å¯†é’¥ï¼ˆè¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…å¯†é’¥ï¼‰ä»¥åŠç”¨äºç»„ç»‡è¿è¡Œè®°å½•çš„é¡¹ç›®åç§°ã€‚\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d0b3ca",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# LangSmithï¼ˆå¯é€‰ï¼šè¿½è¸ªä¸è°ƒè¯•ï¼‰\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ.get(\"LANGCHAIN_API_KEY\", \"YOUR_LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Databricks - Medallion Pipeline\"\n",
        "\n",
        "# Azure OpenAI\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://lixuesenuseno2.openai.azure.com/\"\n",
        "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"LXS-gpt-5\"\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"XX\")  # å»ºè®®é€šè¿‡ç¯å¢ƒå˜é‡æ³¨å…¥çœŸå®å¯†é’¥\n",
        "\n",
        "print(f\"LangSmith configured. Project: '{os.environ['LANGCHAIN_PROJECT']}'\")\n",
        "print(f\"Azure OpenAI endpoint: {os.environ['AZURE_OPENAI_ENDPOINT']}, deployment: {os.environ['AZURE_OPENAI_DEPLOYMENT_NAME']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbed674e",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 3ï¼šå¯¼å…¥ä¾èµ–\n",
        "é€šè¿‡å®Œæ•´çš„åº“å¯¼å…¥æ­å»ºåŸºç¡€ç¯å¢ƒï¼ŒåŒ…æ‹¬ï¼š\n",
        "- LangChain ä¸ Databricks çš„é›†æˆï¼Œç”¨äºåŸºäº LLM çš„æ™ºèƒ½ä½“\n",
        "- LangGraph ç»„ä»¶ï¼Œç”¨äºç¼–æ’æ™ºèƒ½ä½“å·¥ä½œæµ\n",
        "- PySpark SQL èƒ½åŠ›ï¼Œç”¨äºå¯æ‰©å±•çš„æ•°æ®è½¬æ¢\n",
        "- æ ¸å¿ƒæ•°æ®ç§‘å­¦åº“ï¼Œç”¨äºåˆ†æä¸å¯è§†åŒ–\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0241ec9",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "from typing import Annotated, TypedDict, List, Optional\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, regexp_replace, when, lit, md5, concat_ws, expr, to_date, upper, sum as _sum, avg as _avg, count as _count, date_format, year, month\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "364e876d",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 4ï¼šåˆå§‹åŒ– Spark ä¼šè¯ã€LLM ä¸æ•°æ®åº“ Schema\n",
        "\n",
        "æœ¬å•å…ƒæ ¼åˆå§‹åŒ– SparkSessionã€ç”¨äºæ™ºèƒ½ä½“æ“ä½œçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶æŒ‰ç…§ Medallion æ¶æ„æ¨¡å¼åˆ›å»ºæ‰€éœ€çš„æ•°æ®åº“ Schemaï¼ˆBronzeã€Silverã€Goldï¼‰ã€‚åŒæ—¶åœ¨æ­¤å®šä¹‰ log_event å·¥å…·å‡½æ•°ï¼Œç”¨äºåœ¨ç®¡é“æ‰§è¡Œè¿‡ç¨‹ä¸­è¾“å‡ºç»“æ„åŒ–ã€å¸¦é¢œè‰²çš„æ—¥å¿—ã€‚\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f256bba8",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def log_event(level, source, message):\n",
        "    colors = {\"AGENT\": \"\\033[94m\", \"TOOL\": \"\\033[93m\", \"INFO\": \"\\033[92m\", \"ERROR\": \"\\033[91m\", \"GRAPH\": \"\\033[95m\"}\n",
        "    reset_color = \"\\033[0m\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
        "    print(f\"{timestamp} | {colors.get(level, '')}{level:<7}{reset_color} | {colors.get(level, '')}[{source}]{reset_color} {message}\")\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AgenticDataPipeline\").getOrCreate()\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://lixuesenuseno2.openai.azure.com/\"),\n",
        "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"XX\"),\n",
        "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2025-03-01-preview\"),\n",
        "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"LXS-gpt-5\"),\n",
        "    temperature=0.0,\n",
        "    max_tokens=8000,\n",
        ")\n",
        "\n",
        "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ops_bronze\")\n",
        "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ops_silver\")\n",
        "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ops_gold\")\n",
        "\n",
        "log_event(\"INFO\", \"Setup\", \"Environment initialized with Azure OpenAI (LXS-gpt-5)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5238b1a3",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 5ï¼šç”Ÿæˆ Bronze å±‚åŸå§‹æ•°æ®\n",
        "\n",
        "æœ¬å•å…ƒæ ¼ä¸º Bronze å±‚åˆ›å»ºåˆæˆåŸå§‹æ•°æ®ï¼Œåœ¨ ops_bronze schema ä¸‹ç”Ÿæˆ customers_rawã€transactions_rawã€accounts_raw å’Œ opportunities_raw è¡¨çš„æ•°æ®ã€‚è¯¥æ­¥éª¤æ¨¡æ‹Ÿå°†åŸå§‹ã€å¯èƒ½æ‚ä¹±çš„æºæ•°æ®æ‘„å…¥æ•°æ®æ¹–çš„è¿‡ç¨‹ã€‚\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a43f8194",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def create_bronze_data():\n",
        "    log_event(\"INFO\", \"DataGen\", \"æ­£åœ¨ç”Ÿæˆ Bronze å±‚æ•°æ®...\")\n",
        "    \n",
        "    customer_data = []\n",
        "    for i in range(1, 101):\n",
        "        name = f\"FName{i} LName{i}\" if random.random() > 0.1 else None\n",
        "        email = f\"fname{i}.lname{i}@email.com\" if random.random() > 0.1 else \"invalid-email\"\n",
        "        address = f\"{i*10} Main St\" if random.random() > 0.05 else None\n",
        "        customer_data.append((i, name, email, address, f\"2023-01-{random.randint(10, 28)}\"))\n",
        "    \n",
        "    customer_data.extend([\n",
        "        (1, 'John Doe', 'john.doe@email.com', '123 Elm St', '2023-01-15'), \n",
        "        (3, 'Peter Jones', 'peter.jones.dup@email.com', '789 Pine Ln', '2023-01-17')\n",
        "    ])\n",
        "    spark.createDataFrame(customer_data, [\"customer_id\", \"name\", \"email\", \"address\", \"join_date\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.customers_raw\")\n",
        "\n",
        "    transactions_data = []\n",
        "    for i in range(1, 201):\n",
        "        cust_id = random.randint(1, 110)\n",
        "        qty = random.randint(-5, 50)\n",
        "        amount = f\"${random.uniform(5, 1000):.2f}\" if random.random() > 0.05 else None\n",
        "        transactions_data.append((100+i, cust_id, qty, amount, f\"2023-02-{random.randint(1, 28)}\"))\n",
        "    spark.createDataFrame(transactions_data, [\"transaction_id\", \"customer_id\", \"quantity\", \"amount\", \"transaction_date\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.transactions_raw\")\n",
        "\n",
        "    accounts_data = [('ACC{:03d}'.format(i), f'GlobalCorp-{i}', random.choice(['Technology', 'Healthcare', 'Finance', 'TECH', None]), random.choice(['USA', 'UK', 'Germany'])) for i in range(1, 51)]\n",
        "    accounts_data.append(('ACC001', 'Global Corporation', 'Tech', 'USA'))\n",
        "    spark.createDataFrame(accounts_data, [\"account_id\", \"account_name\", \"industry\", \"region\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.accounts_raw\")\n",
        "    \n",
        "    opportunities_data = [(f'OPP{i:03d}', f\"ACC{random.randint(1, 55):03d}\", random.randint(-1000, 200000), '2024-07-15', random.choice(['Closed Won', 'Negotiation', 'Proposal', 'Qualification', 'Closed Lost'])) for i in range(1, 151)]\n",
        "    spark.createDataFrame(opportunities_data, [\"opportunity_id\", \"account_id\", \"value\", \"close_date\", \"stage\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.opportunities_raw\")\n",
        "    \n",
        "    log_event(\"INFO\", \"DataGen\", \"Bronze data creation complete\")\n",
        "    display(spark.sql(\"SHOW TABLES IN ops_bronze\"))\n",
        "\n",
        "create_bronze_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1925d440",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 6ï¼šå®šä¹‰æ™ºèƒ½ä½“å·¥å…·\n",
        "\n",
        "æœ¬å•å…ƒæ ¼å®šä¹‰ LangGraph å·¥ä½œæµä¸­æ™ºèƒ½ä½“å¯è°ƒç”¨çš„å·¥å…·å‡½æ•°ï¼š\n",
        "\n",
        "- **get_table_info**ï¼šå¯¹æŒ‡å®šè¡¨è¿›è¡Œç”»åƒï¼Œæä¾› Schemaã€è¡Œæ•°ä¸æ•°æ®é¢„è§ˆã€‚\n",
        "- **execute_pyspark_code**ï¼šåœ¨ Spark ç¯å¢ƒä¸­å®‰å…¨æ‰§è¡Œç”Ÿæˆçš„ PySpark ä»£ç ï¼Œå¹¶å¤„ç†é”™è¯¯ã€‚\n",
        "- **create_notebook_visualization**ï¼šåœ¨ Databricks ç¬”è®°æœ¬å†…ç›´æ¥ç”Ÿæˆå¹¶å±•ç¤ºåŸºç¡€å¯è§†åŒ–ã€‚\n",
        "\n",
        "è¿™äº›å·¥å…·ä½¿ LLM æ™ºèƒ½ä½“èƒ½å¤Ÿä¸æ•°æ®åŠç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba52b67d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def get_table_info(table_name: str) -> str:\n",
        "    \"\"\"å¯¹è¡¨è¿›è¡Œç”»åƒï¼šåŒ…å« Schemaã€ç»Ÿè®¡ä¿¡æ¯ä¸æ•°æ®é¢„è§ˆï¼Œç”¨äºè½¬æ¢è§„åˆ’ã€‚\"\"\"\n",
        "    try:\n",
        "        log_event(\"TOOL\", \"get_table_info\", f\"æ­£åœ¨ç”»åƒ: {table_name}\")\n",
        "        df = spark.table(table_name)\n",
        "        schema_str = \"\\n\".join([f\"- {field.name}: {str(field.dataType)}\" for field in df.schema.fields])\n",
        "        count = df.count()\n",
        "        preview = df.limit(3).toPandas().to_string()\n",
        "        return f\"TABLE: {table_name}\\nROW_COUNT: {count}\\n\\nSCHEMA:\\n{schema_str}\\n\\nSAMPLE_DATA:\\n{preview}\"\n",
        "    except Exception as e:\n",
        "        return f\"é”™è¯¯: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def execute_pyspark_code(code: str) -> str:\n",
        "    \"\"\"åœ¨å…·å¤‡é”™è¯¯å¤„ç†ä¸æ ¡éªŒçš„å‰æä¸‹æ‰§è¡Œ PySpark è½¬æ¢ä»£ç ã€‚\"\"\"\n",
        "    try:\n",
        "        log_event(\"TOOL\", \"execute_pyspark_code\", \"æ­£åœ¨æ‰§è¡Œè½¬æ¢...\")\n",
        "        exec_globals = {\n",
        "            'spark': spark, 'col': col, 'regexp_replace': regexp_replace, 'when': when, 'lit': lit,\n",
        "            'md5': md5, 'concat_ws': concat_ws, 'expr': expr, 'to_date': to_date, 'upper': upper,\n",
        "            '_sum': _sum, '_avg': _avg, '_count': _count, 'date_format': date_format, 'year': year, 'month': month, 'Window': Window\n",
        "        }\n",
        "        exec(code, exec_globals)\n",
        "        log_event(\"TOOL\", \"execute_pyspark_code\", \"æ‰§è¡ŒæˆåŠŸ\")\n",
        "        return json.dumps({\"status\": \"success\", \"error\": None})\n",
        "    except Exception as e:\n",
        "        error_msg = f\"æ‰§è¡Œé”™è¯¯: {str(e)}\"\n",
        "        log_event(\"ERROR\", \"execute_pyspark_code\", error_msg)\n",
        "        return json.dumps({\"status\": \"error\", \"error\": error_msg})\n",
        "        \n",
        "@tool\n",
        "def create_notebook_visualization(table_name: str, plot_type: str, x_col: str, y_col: str, title: str) -> str:\n",
        "    \"\"\"åˆ›å»ºä¼˜åŒ–çš„ä»ªè¡¨ç›˜å¯è§†åŒ–ï¼Œè‡ªåŠ¨æ’åºä¸é™åˆ¶æ•°æ®ä»¥æå‡æ€§èƒ½ã€‚\"\"\"\n",
        "    try:\n",
        "        log_event(\"TOOL\", \"create_visualization\", f\"æ­£åœ¨åˆ›å»º: {title}\")\n",
        "        df = spark.table(table_name)\n",
        "        if plot_type == 'bar' and 'Top' in title:\n",
        "            df_display = df.orderBy(col(y_col).desc()).limit(5)\n",
        "        else:\n",
        "            df_display = df.orderBy(x_col)\n",
        "        print(f\"\\n=== DASHBOARD COMPONENT: {title} ===\")\n",
        "        display(df_display)\n",
        "        return f\"SUCCESS: '{title}' visualization created and displayed\"\n",
        "    except Exception as e:\n",
        "        return f\"VISUALIZATION_ERROR: {str(e)}\"\n",
        "\n",
        "all_tools = [get_table_info, execute_pyspark_code, create_notebook_visualization]\n",
        "tool_node = ToolNode(all_tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a44a1d",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 7ï¼šå®šä¹‰æ™ºèƒ½ä½“çŠ¶æ€ã€èŠ‚ç‚¹ä¸å›¾å·¥ä½œæµ\n",
        "\n",
        "è¿™æ˜¯æ™ºèƒ½ä½“ç®¡é“çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚\n",
        "\n",
        "### AgentStateï¼ˆæ™ºèƒ½ä½“çŠ¶æ€ï¼‰\n",
        "å®šä¹‰åœ¨ LangGraph å·¥ä½œæµä¸­åœ¨æ™ºèƒ½ä½“ä¹‹é—´ä¼ é€’çš„å…±äº«çŠ¶æ€å¯¹è±¡ï¼ŒåŒ…æ‹¬æ¶ˆæ¯ã€å½“å‰ä»»åŠ¡ã€PySpark ä»£ç ã€å®¡æŸ¥åé¦ˆã€æ‰§è¡Œé”™è¯¯ä¸é‡è¯•æ¬¡æ•°ã€‚\n",
        "\n",
        "### æ™ºèƒ½ä½“å‡½æ•°\n",
        "\n",
        "- **planner_agent**ï¼šç”Ÿæˆè¯¦ç»†çš„æ•°æ®è½¬æ¢æ–¹æ¡ˆ\n",
        "- **code_generator_agent**ï¼šæ ¹æ®æ–¹æ¡ˆä¸åé¦ˆç¼–å†™ PySpark ä»£ç \n",
        "- **code_reviewer_agent**ï¼šå¯¹ç”Ÿæˆçš„ä»£ç è¿›è¡Œè´¨é‡ä¸æ­£ç¡®æ€§å®¡æŸ¥\n",
        "- **prepare_for_execution**ï¼šå°† PySpark ä»£ç æ ¼å¼åŒ–ä¸ºå¯ä¾›å·¥å…·æ‰§è¡Œçš„å½¢å¼\n",
        "\n",
        "### æ¡ä»¶è¾¹\n",
        "- **after_review_decider**ï¼šæ ¹æ®ä»£ç å®¡æŸ¥åé¦ˆè¿›è¡Œè·¯ç”±\n",
        "- **after_execution_decider**ï¼šæ ¹æ®æ‰§è¡Œç»“æœè¿›è¡Œè·¯ç”±\n",
        "äºŒè€…å…±åŒå®ç°è‡ªä¸»è‡ªçº å¾ªç¯ã€‚\n",
        "\n",
        "### å·¥ä½œæµå®šä¹‰\n",
        "StateGraph å®šä¹‰ Medallion ç®¡é“å·¥ä½œæµçš„èŠ‚ç‚¹ï¼ˆæ™ºèƒ½ä½“ä¸å·¥å…·ï¼‰ä¸è¾¹ï¼ˆçŠ¶æ€è½¬ç§»ï¼‰ã€‚\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc59496",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, lambda x, y: x + y]\n",
        "    current_task: str\n",
        "    pyspark_code: Optional[str]\n",
        "    review_feedback: Optional[str]\n",
        "    execution_error: Optional[str]\n",
        "    retry_count: int\n",
        "\n",
        "def planner_agent(state: AgentState):\n",
        "    log_event(\"AGENT\", \"Planner\", \"æ­£åœ¨åˆ¶å®šè½¬æ¢ç­–ç•¥\")\n",
        "    \n",
        "    system_prompt = \"\"\"You are an expert data architect specializing in Databricks Medallion architecture. \n",
        "    \n",
        "    OBJECTIVES:\n",
        "    - Create precise, executable PySpark transformation plans\n",
        "    - Ensure data quality, deduplication, and proper type casting\n",
        "    - Follow medallion best practices (Bronze=raw, Silver=cleaned, Gold=aggregated)\n",
        "    - Design for scalability and performance optimization\n",
        "    \n",
        "    OUTPUT FORMAT:\n",
        "    Provide a detailed, step-by-step technical plan with:\n",
        "    1. Data profiling requirements\n",
        "    2. Specific transformation logic\n",
        "    3. Quality checks and validation rules\n",
        "    4. Performance optimization strategies\"\"\"\n",
        "    \n",
        "    user_prompt = f\"\"\"è½¬æ¢ä»»åŠ¡: {state['current_task']}\n",
        "    \n",
        "    è¯·åˆ¶å®šå®Œæ•´çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œæ¶µç›–ï¼š\n",
        "    - æ•°æ®è´¨é‡é—®é¢˜ä¸ä¿®å¤\n",
        "    - Schema æ ‡å‡†åŒ–ä¸ç±»å‹è½¬æ¢\n",
        "    - å»é‡ç­–ç•¥\n",
        "    - ä¸šåŠ¡è§„åˆ™å®ç°\n",
        "    - æ€§èƒ½ä¼˜åŒ–æ‰‹æ®µ\"\"\"\n",
        "    \n",
        "    messages = [HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")]\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"messages\": [AIMessage(content=response.content, name=\"PlannerAgent\")]}\n",
        "\n",
        "def code_generator_agent(state: AgentState):\n",
        "    log_event(\"AGENT\", \"CodeGenerator\", \"æ­£åœ¨ç”Ÿæˆ PySpark å®ç°\")\n",
        "    \n",
        "    plan = next((msg.content for msg in state['messages'] if isinstance(msg, AIMessage) and msg.name == \"PlannerAgent\"), \"\")\n",
        "    \n",
        "    context_prompt = \"\"\n",
        "    if state.get('review_feedback') and \"APPROVED\" not in state['review_feedback'].upper():\n",
        "        context_prompt = f\"\\n\\néœ€é‡‡çº³çš„å®¡æŸ¥åé¦ˆ:\\n{state['review_feedback']}\"\n",
        "    elif state.get('execution_error'):\n",
        "        context_prompt = f\"\\n\\néœ€ä¿®å¤çš„æ‰§è¡Œé”™è¯¯:\\n{state['execution_error']}\"\n",
        "\n",
        "    system_prompt = \"\"\"You are a senior PySpark developer with expertise in Databricks optimization.\n",
        "\n",
        "    CODING REQUIREMENTS:\n",
        "    - Generate complete, executable PySpark code only\n",
        "    - Use explicit imports for all functions\n",
        "    - Implement robust error handling and data validation\n",
        "    - Apply performance optimizations (caching, partitioning)\n",
        "    - Follow PySpark best practices for large-scale data processing\n",
        "    - Use .mode(\"overwrite\").saveAsTable() for all writes\n",
        "    \n",
        "    CRITICAL CONSTRAINTS:\n",
        "    - NO explanatory text, comments, or markdown\n",
        "    - SparkSession 'spark' is pre-initialized\n",
        "    - Code must be production-ready and optimized\n",
        "    - Handle edge cases and null values appropriately\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"å®æ–½æ–¹æ¡ˆ:\\n{plan}{context_prompt}\n",
        "    \n",
        "    è¯·ç”Ÿæˆå®Œæ•´çš„ PySpark è½¬æ¢ä»£ç ï¼Œè¦æ±‚ï¼š\n",
        "    1. å®ç°æ–¹æ¡ˆä¸­çš„å…¨éƒ¨è¦æ±‚\n",
        "    2. ç¨³å¥å¤„ç†æ•°æ®è´¨é‡é—®é¢˜\n",
        "    3. å…¼é¡¾æ€§èƒ½ä¸å¯æ‰©å±•æ€§\n",
        "    4. è¾“å‡ºå¹²å‡€ã€å¯é çš„ç»“æœè¡¨\"\"\"\n",
        "\n",
        "    messages = [HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")]\n",
        "    response = llm.invoke(messages)\n",
        "    clean_code = response.content.strip().replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
        "    return {\"pyspark_code\": clean_code, \"execution_error\": None, \"review_feedback\": None}\n",
        "\n",
        "def code_reviewer_agent(state: AgentState):\n",
        "    log_event(\"AGENT\", \"CodeReviewer\", \"æ­£åœ¨è¿›è¡Œè´¨é‡å®¡æŸ¥\")\n",
        "    \n",
        "    plan = next((msg.content for msg in state['messages'] if isinstance(msg, AIMessage) and msg.name == \"PlannerAgent\"), \"\")\n",
        "    \n",
        "    system_prompt = \"\"\"You are a senior data engineering QA specialist with deep PySpark expertise.\n",
        "\n",
        "    REVIEW CRITERIA:\n",
        "    - Code completeness and correctness\n",
        "    - Performance optimization implementation\n",
        "    - Data quality and validation logic\n",
        "    - Error handling robustness\n",
        "    - Adherence to transformation requirements\n",
        "    - Production readiness standards\n",
        "\n",
        "    RESPONSE FORMAT:\n",
        "    - If code meets ALL requirements: respond with exactly \"APPROVED\"\n",
        "    - If issues exist: respond with \"REJECTION_REASON: [specific actionable feedback]\"\n",
        "    \n",
        "    Be thorough but decisive. Code must be production-quality.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"åŸå§‹æ–¹æ¡ˆ:\\n{plan}\\n\\nå®ç°ä»£ç :\\n```python\\n{state['pyspark_code']}\\n```\n",
        "    \n",
        "    è¯·è¯„ä¼°ä»£ç æ˜¯å¦å®Œæ•´å®ç°æ–¹æ¡ˆè¦æ±‚å¹¶è¾¾åˆ°ç”Ÿäº§çº§æ ‡å‡†ã€‚\"\"\"\n",
        "\n",
        "    messages = [HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")]\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"review_feedback\": response.content}\n",
        "\n",
        "def prepare_for_execution(state: AgentState):\n",
        "    log_event(\"AGENT\", \"ExecutorPrep\", \"å‡†å¤‡æ‰§è¡Œä»£ç \")\n",
        "    tool_call_message = AIMessage(\n",
        "        content=\"\", \n",
        "        tool_calls=[{\n",
        "            'name': 'execute_pyspark_code', \n",
        "            'args': {'code': state.get('pyspark_code')}, \n",
        "            'id': f'exec_{datetime.now().isoformat()}'\n",
        "        }]\n",
        "    )\n",
        "    return {\"messages\": [tool_call_message]}\n",
        "\n",
        "def after_review_decider(state: AgentState):\n",
        "    feedback = state.get('review_feedback', '')\n",
        "    if \"APPROVED\" in feedback.upper():\n",
        "        log_event(\"GRAPH\", \"Router\", \"ä»£ç å·²é€šè¿‡ â†’ æ‰§è¡Œ\")\n",
        "        return \"prepare_for_execution\"\n",
        "    else:\n",
        "        log_event(\"GRAPH\", \"Router\", f\"ä»£ç æœªé€šè¿‡ â†’ ä¿®è®¢ï¼ˆç¬¬ {state.get('retry_count', 0) + 1} æ¬¡å°è¯•ï¼‰\")\n",
        "        return \"revise_code\"\n",
        "\n",
        "def after_execution_decider(state: AgentState):\n",
        "    last_message = state['messages'][-1]\n",
        "    execution_result = json.loads(last_message.content)\n",
        "    \n",
        "    if execution_result[\"status\"] == \"success\":\n",
        "        log_event(\"GRAPH\", \"Router\", \"æ‰§è¡ŒæˆåŠŸ â†’ å®Œæˆ\")\n",
        "        return END\n",
        "    elif state.get('retry_count', 0) < 3:\n",
        "        log_event(\"GRAPH\", \"Router\", \"æ‰§è¡Œå¤±è´¥ â†’ é‡è¯•\")\n",
        "        state['execution_error'] = execution_result[\"error\"]\n",
        "        return \"code_generator\"\n",
        "    else:\n",
        "        log_event(\"ERROR\", \"Router\", \"å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° â†’ ä¸­æ­¢\")\n",
        "        return END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"planner\", planner_agent)\n",
        "workflow.add_node(\"code_generator\", code_generator_agent)\n",
        "workflow.add_node(\"code_reviewer\", code_reviewer_agent)\n",
        "workflow.add_node(\"revise_code_node\", lambda state: {\"retry_count\": state.get('retry_count', 0) + 1})\n",
        "workflow.add_node(\"prepare_for_execution\", prepare_for_execution)\n",
        "workflow.add_node(\"executor\", tool_node)\n",
        "\n",
        "workflow.set_entry_point(\"planner\")\n",
        "workflow.add_edge(\"planner\", \"code_generator\")\n",
        "workflow.add_edge(\"code_generator\", \"code_reviewer\")\n",
        "workflow.add_conditional_edges(\"code_reviewer\", after_review_decider, {\n",
        "    \"prepare_for_execution\": \"prepare_for_execution\", \n",
        "    \"revise_code\": \"revise_code_node\"\n",
        "})\n",
        "workflow.add_edge(\"revise_code_node\", \"code_generator\")\n",
        "workflow.add_edge(\"prepare_for_execution\", \"executor\")\n",
        "workflow.add_conditional_edges(\"executor\", after_execution_decider, {\n",
        "    \"code_generator\": \"code_generator\", \n",
        "    \"__end__\": END\n",
        "})\n",
        "\n",
        "app = workflow.compile()\n",
        "log_event(\"INFO\", \"Setup\", \"Enhanced LangGraph pipeline compiled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac837680",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def create_workflow_diagram():\n",
        "    \"\"\"å½“å¯è§†åŒ–ä¸å¯ç”¨æ—¶ï¼Œç”ŸæˆåŸºäºæ–‡æœ¬çš„å·¥ä½œæµå›¾ä½œä¸ºå¤‡é€‰\"\"\"\n",
        "    diagram = \"\"\"\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚   PLANNER   â”‚\n",
        "    â”‚   AGENT     â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "          â”‚\n",
        "          â–¼\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚    CODE     â”‚\n",
        "    â”‚ GENERATOR   â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "          â”‚\n",
        "          â–¼\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚    CODE     â”‚\n",
        "    â”‚  REVIEWER   â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "          â”‚\n",
        "          â–¼\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚  APPROVED?  â”‚â”€â”€â”€â–ºâ”‚   REVISE    â”‚\n",
        "    â”‚             â”‚    â”‚    CODE     â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "          â”‚                  â”‚\n",
        "          â–¼                  â”‚\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
        "    â”‚  EXECUTE    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "    â”‚    CODE     â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "          â”‚\n",
        "          â–¼\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚  SUCCESS?   â”‚\n",
        "    â”‚             â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "    \"\"\"\n",
        "    print(\"=== LangGraph å·¥ä½œæµå›¾ ===\")\n",
        "    print(diagram)\n",
        "\n",
        "try:\n",
        "    # Try to create visual graph with improved error handling\n",
        "    try:\n",
        "        # First try with mermaid text output (safer fallback)\n",
        "        mermaid_code = app.get_graph().draw_mermaid()\n",
        "        print(\"=== WORKFLOW MERMAID DIAGRAM ===\")\n",
        "        print(mermaid_code)\n",
        "    except Exception as mermaid_error:\n",
        "        log_event(\"INFO\", \"Visualization\", f\"Mermaid generation failed: {mermaid_error}\")\n",
        "        \n",
        "    # Try PNG generation with better error handling\n",
        "    try:\n",
        "        from langgraph.graph.graph import CompiledGraph\n",
        "        if hasattr(app.get_graph(), 'draw_ascii'):\n",
        "            ascii_graph = app.get_graph().draw_ascii()\n",
        "            print(\"=== WORKFLOW ASCII DIAGRAM ===\")\n",
        "            print(ascii_graph)\n",
        "        else:\n",
        "            create_workflow_diagram()\n",
        "    except Exception as png_error:\n",
        "        log_event(\"INFO\", \"Visualization\", f\"PNG visualization unavailable: {png_error}\")\n",
        "        create_workflow_diagram()\n",
        "        \n",
        "except Exception as e:\n",
        "    log_event(\"INFO\", \"Visualization\", \"Using text-based workflow diagram\")\n",
        "    create_workflow_diagram()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72ec4c76",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 9ï¼šæ‰§è¡Œ Bronze åˆ° Silver çš„è½¬æ¢\n",
        "\n",
        "æœ¬å•å…ƒæ ¼å¯åŠ¨ Medallion ç®¡é“çš„ç¬¬ä¸€ä¸ªä¸»è¦é˜¶æ®µï¼šå°† Bronze å±‚çš„åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¸…æ´—ã€æ ‡å‡†åŒ–åçš„ Silver å±‚ã€‚å®ƒä¼šéå†é¢„å®šä¹‰çš„è½¬æ¢ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡æè¿°å¯¹ç‰¹å®šè¡¨ï¼ˆå®¢æˆ·ã€äº¤æ˜“ã€è´¦æˆ·ã€å•†æœºï¼‰è¿›è¡Œæ•°æ®æ¸…æ´—ä¸æ ¡éªŒçš„è¦æ±‚ã€‚execute_pipeline_task å‡½æ•°ä¸ºæ¯ä¸ªä»»åŠ¡ç¼–æ’æ™ºèƒ½ä½“å·¥ä½œæµã€‚\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf5c124",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def execute_pipeline_task(task_description):\n",
        "    task_name = task_description.strip().splitlines()[0][:50]\n",
        "    log_event(\"INFO\", \"Pipeline\", f\"æ­£åœ¨æ‰§è¡Œ: {task_name}\")\n",
        "    \n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=\"åˆå§‹åŒ–è½¬æ¢ç®¡é“\")], \n",
        "        \"current_task\": task_description, \n",
        "        \"retry_count\": 0\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        for state_update in app.stream(initial_state, {\"recursion_limit\": 20}):\n",
        "            node_name = list(state_update.keys())[0]\n",
        "            log_event(\"GRAPH\", \"Flow\", f\"å·²å®Œæˆ: {node_name}\")\n",
        "        \n",
        "        log_event(\"INFO\", \"Pipeline\", f\"å·²å®Œæˆ: {task_name}\")\n",
        "    except Exception as e:\n",
        "        log_event(\"ERROR\", \"Pipeline\", f\"ä»»åŠ¡å¤±è´¥: {task_name} - {str(e)}\")\n",
        "    \n",
        "    time.sleep(1)\n",
        "\n",
        "log_event(\"INFO\", \"PIPELINE\", \"===== BRONZE â†’ SILVER TRANSFORMATIONS =====\")\n",
        "\n",
        "bronze_to_silver_transformations = [\n",
        "    \"\"\"CUSTOMER DATA CLEANSING: ops_bronze.customers_raw â†’ ops_silver.customers_cleaned\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Remove duplicate customer_id records (keep first occurrence)  \n",
        "    - Fill null 'name' values with 'Unknown Customer'\n",
        "    - Validate email format using regex pattern '.+@.+\\\\..+' \n",
        "    - Set invalid emails to null\n",
        "    - Convert join_date string to proper DateType\n",
        "    - Filter out records with null email OR null address\n",
        "    - Add data quality flags for tracking\"\"\",\n",
        "    \n",
        "    \"\"\"TRANSACTION DATA STANDARDIZATION: ops_bronze.transactions_raw â†’ ops_silver.transactions_cleaned\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Deduplicate on transaction_id (keep first record)\n",
        "    - Clean amount field: remove '$' prefix and convert to Decimal(10,2)\n",
        "    - Filter out negative quantity transactions\n",
        "    - Filter out null or zero amount transactions  \n",
        "    - Restrict to customer_id <= 100 (valid customers only)\n",
        "    - Convert transaction_date to DateType\n",
        "    - Add calculated fields for analysis\"\"\",\n",
        "    \n",
        "    \"\"\"ACCOUNT DATA NORMALIZATION: ops_bronze.accounts_raw â†’ ops_silver.accounts_cleaned\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Deduplicate by account_id, preserving first record\n",
        "    - Standardize industry values to uppercase\n",
        "    - Map 'TECH' industry to 'TECHNOLOGY' \n",
        "    - Replace null industry with 'NOT_SPECIFIED'\n",
        "    - Validate account_id format consistency\n",
        "    - Add data lineage tracking columns\"\"\",\n",
        "    \n",
        "    \"\"\"OPPORTUNITY DATA VALIDATION: ops_bronze.opportunities_raw â†’ ops_silver.opportunities_cleaned\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Cast opportunity value to Decimal(14,2) with proper handling\n",
        "    - Filter out opportunities with value <= 0\n",
        "    - Validate account_id matches pattern 'ACC###'\n",
        "    - Convert close_date string to DateType\n",
        "    - Standardize stage values with proper casing\n",
        "    - Add business rule validation flags\"\"\"\n",
        "]\n",
        "\n",
        "for task in bronze_to_silver_transformations:\n",
        "    execute_pipeline_task(task)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76a96a95",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 10ï¼šæ‰§è¡Œ Silver åˆ° Gold çš„èšåˆ\n",
        "\n",
        "æœ¬å•å…ƒæ ¼æ‰§è¡Œ Medallion ç®¡é“çš„ç¬¬äºŒä¸ªä¸»è¦é˜¶æ®µï¼šå°† Silver å±‚çš„æ•°æ®èšåˆå¹¶ä¸°å¯Œåˆ° Gold å±‚ã€‚å®ƒå®šä¹‰å¹¶æ‰§è¡Œåˆ›å»ºåˆ†æè¡¨ï¼ˆå¦‚ customer_spendingã€account_performanceã€monthly_sales_summaryï¼‰çš„ä»»åŠ¡ã€‚è¿™äº›èšåˆä¸ºå•†ä¸šæ™ºèƒ½ä¸æŠ¥è¡¨æä¾›æ•°æ®å‡†å¤‡ã€‚\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42cd24b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "log_event(\"INFO\", \"PIPELINE\", \"===== SILVER â†’ GOLD AGGREGATIONS =====\")\n",
        "\n",
        "silver_to_gold_aggregations = [\n",
        "    \"\"\"CUSTOMER SPENDING ANALYTICS: Create ops_gold.customer_spending\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Join ops_silver.customers_cleaned with ops_silver.transactions_cleaned\n",
        "    - Group by customer_id and customer name\n",
        "    - Calculate total_spent (sum of amounts) with null handling\n",
        "    - Calculate total_transactions (count of transactions)\n",
        "    - Calculate average_transaction_value \n",
        "    - Add customer spending tier classification\n",
        "    - Order by total_spent descending for performance\"\"\",\n",
        "    \n",
        "    \"\"\"ACCOUNT PERFORMANCE METRICS: Create ops_gold.account_performance  \n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Join ops_silver.accounts_cleaned with ops_silver.opportunities_cleaned\n",
        "    - Group by account_id, account_name, and industry\n",
        "    - Calculate total_pipeline_value (sum of all opportunity values)\n",
        "    - Calculate won_value (sum where stage = 'Closed Won')\n",
        "    - Calculate win_rate as percentage of closed won vs total closed\n",
        "    - Count open_opportunities (non-closed stages)\n",
        "    - Add performance ranking within industry\"\"\",\n",
        "    \n",
        "    \"\"\"MONTHLY SALES TRENDS: Create ops_gold.monthly_sales_summary\n",
        "    \n",
        "    REQUIREMENTS:\n",
        "    - Source from ops_silver.transactions_cleaned\n",
        "    - Extract year-month from transaction_date as 'YYYY-MM' format\n",
        "    - Group by month period  \n",
        "    - Calculate monthly_revenue (sum of amounts)\n",
        "    - Calculate monthly_transaction_count\n",
        "    - Calculate average_transaction_size per month\n",
        "    - Add month-over-month growth calculations\n",
        "    - Order chronologically for time series analysis\"\"\"\n",
        "]\n",
        "\n",
        "for task in silver_to_gold_aggregations:\n",
        "    execute_pipeline_task(task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e368e267",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "log_event(\"INFO\", \"PIPELINE\", \"===== BUSINESS INTELLIGENCE DASHBOARD =====\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a91e03",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## å•å…ƒæ ¼ 12ï¼šç”Ÿæˆå¹¶æ ¡éªŒä»ªè¡¨ç›˜å¯è§†åŒ–\n",
        "\n",
        "æœ¬å•å…ƒæ ¼åˆ©ç”¨ bi_agentï¼ˆä½¿ç”¨ Notebook_visualization å·¥å…·ï¼‰ç”Ÿæˆå…³é”®å•†ä¸šæ™ºèƒ½ä»ªè¡¨ç›˜ç»„ä»¶ï¼Œéšåå¯¹ Gold å±‚è¡¨è¿›è¡Œæœ€ç»ˆæ•°æ®æ ¡éªŒï¼Œé€šè¿‡å±•ç¤ºè®°å½•æ•°ä¸ Top N è¡Œï¼Œç¡®ä¿ç®¡é“è¾“å‡ºç¬¦åˆé¢„æœŸã€‚\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659bcaf0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "dashboard_prompt = \"\"\"ä½ æ˜¯å•†ä¸šæ™ºèƒ½ï¼ˆBIï¼‰ä¸“å®¶ã€‚è¯·ä½¿ç”¨ create_notebook_visualization å·¥å…·åˆ›å»ºé«˜ç®¡ä»ªè¡¨ç›˜å¯è§†åŒ–ã€‚\n",
        "\n",
        "ä»ªè¡¨ç›˜è¦æ±‚:\n",
        "1. å®¢æˆ·åˆ†æï¼šæŒ‰æ€»æ¶ˆè´¹é¢ Top 5 å®¢æˆ·ï¼ˆæ¥è‡ª ops_gold.customer_spending çš„æŸ±çŠ¶å›¾ï¼‰\n",
        "2. è¡Œä¸šç»©æ•ˆï¼šæŒ‰è¡Œä¸šçš„ç®¡é“é‡‘é¢åˆ†å¸ƒï¼ˆæ¥è‡ª ops_gold.account_performance çš„æŸ±çŠ¶å›¾ï¼‰\n",
        "3. æ”¶å…¥è¶‹åŠ¿ï¼šæœˆåº¦æ”¶å…¥éšæ—¶é—´å˜åŒ–ï¼ˆæ¥è‡ª ops_gold.monthly_sales_summary çš„æŠ˜çº¿å›¾ï¼‰\n",
        "\n",
        "æ‰§è¡Œä»¥ä¸Šä¸‰ä¸ªå¯è§†åŒ–ä»¥å®Œæˆé«˜ç®¡ä»ªè¡¨ç›˜ã€‚\"\"\"\n",
        "\n",
        "bi_agent = llm.bind_tools(all_tools)\n",
        "dashboard_response = bi_agent.invoke(dashboard_prompt)\n",
        "\n",
        "if dashboard_response.tool_calls:\n",
        "    for tool_call in dashboard_response.tool_calls:\n",
        "        tool_function = {t.name: t for t in all_tools}[tool_call['name']]\n",
        "        result = tool_function.invoke(tool_call['args'])\n",
        "        log_event(\"INFO\", \"Dashboard\", f\"å·²åˆ›å»ºå¯è§†åŒ–: {tool_call['args'].get('title', 'Unknown')}\")\n",
        "else:\n",
        "    log_event(\"ERROR\", \"Dashboard\", \"æœªèƒ½ç”Ÿæˆä»ªè¡¨ç›˜å¯è§†åŒ–\")\n",
        "\n",
        "log_event(\"INFO\", \"VALIDATION\", \"===== æœ€ç»ˆæ•°æ®æ ¡éªŒ =====\")\n",
        "\n",
        "validation_tables = [\n",
        "    \"ops_gold.customer_spending\",\n",
        "    \"ops_gold.account_performance\", \n",
        "    \"ops_gold.monthly_sales_summary\"\n",
        "]\n",
        "\n",
        "for table in validation_tables:\n",
        "    try:\n",
        "        print(f\"\\n=== æœ€ç»ˆæ ¡éªŒ: {table} ===\")\n",
        "        df = spark.table(table)\n",
        "        print(f\"Record Count: {df.count()}\")\n",
        "        if \"customer_spending\" in table:\n",
        "            display(df.orderBy(col(\"total_spent\").desc()).limit(10))\n",
        "        elif \"account_performance\" in table:\n",
        "            display(df.orderBy(col(\"total_pipeline_value\").desc()).limit(10))\n",
        "        else:\n",
        "            display(df.orderBy(\"month\").limit(12))\n",
        "    except Exception as e:\n",
        "        log_event(\"ERROR\", \"Validation\", f\"æ ¡éªŒ {table} å¤±è´¥: {str(e)}\")\n",
        "\n",
        "log_event(\"INFO\", \"COMPLETION\", \"===== Medallion ç®¡é“æ‰§è¡Œå®Œæˆ =====\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f548d91",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "print(\"\\nğŸ‰ SUCCESS: Databricks Medallion Architecture pipeline completed successfully!\")\n",
        "print(\"âœ… All Bronze â†’ Silver â†’ Gold transformations executed\")\n",
        "print(\"âœ… Business Intelligence dashboard components created\")\n",
        "print(\"âœ… Data quality validation completed\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
